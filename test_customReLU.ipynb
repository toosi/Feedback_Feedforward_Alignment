{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import init\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import warnings\n",
    "from torch.nn.modules.utils import _single, _pair\n",
    "import math\n",
    "import copy\n",
    "# do crazy end-to-end optimization of backward path with MNIST/CIFAR10 and nn.linear    \n",
    "# when calling loss.backward both gradinet_weight and grad_weight_feedback are being computed in customized modules\n",
    "# first control against BP and regular modules\n",
    "# Linear from here :https://pytorch.org/docs/stable/notes/extending.html\n",
    "# Conv from here: https://github.com/pytorch/pytorch/blob/master/torch/nn/grad.py    \n",
    "# autograd : https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward  \n",
    "\n",
    "import torch.autograd as autograd\n",
    "class ReLUGrad(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLUGrad, self).__init__()\n",
    "    def forward(self, grad_output, input):\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "class LinearFunction2(autograd.Function):\n",
    "\n",
    "    \"\"\"\n",
    "    Autograd function for a linear layer with asymmetric feedback and feedforward pathways\n",
    "    forward  : weight\n",
    "    backward : weight_feedback\n",
    "    bias is set to None for now\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    # same as reference linear function, but with additional fa tensor for backward\n",
    "    def forward(context, input, input2, weight, weight_feedback, bias=None, algorithm='BP'):\n",
    "        context.save_for_backward(input,input2,  weight, weight_feedback, bias)\n",
    "        context.algorithm = algorithm\n",
    "        output = input.mm(weight.t())\n",
    "\n",
    "        if bias is not None:\n",
    "            output  += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, grad_output):\n",
    "        input,input2, weight, weight_feedback, bias, algorithm_id = context.saved_tensors\n",
    "        grad_input = grad_input2 = grad_weight = grad_weight_feedback = grad_bias = None\n",
    "\n",
    "        if context.needs_input_grad[0]:\n",
    "            # all of the logic of FA resides in this one line\n",
    "            # calculate the gradient of input with fixed fa tensor, rather than the \"correct\" model weight\n",
    "            grad_input = grad_output.mm(weight_feedback)\n",
    "        if context.needs_input_grad[2]:\n",
    "            # grad for weight with FA'ed grad_output from downstream layer\n",
    "            # it is same with original linear function\n",
    "            #grad_weight = grad_output.t().mm(input) # (sorta)Hebbian update for forward\n",
    "            grad_weight = grad_output.t().mm(input2) # using the second input for computing the gradients\n",
    "            \n",
    "        if context.needs_input_grad[3]:\n",
    "            # only YY needs gradients for backward weights\n",
    "\n",
    "            #grad_weight_feedback = grad_output.t().mm(input)  # (sorta)Hebbian update for backward\n",
    "            grad_weight_feedback = grad_output.t().mm(input2)   # using the second input for computing the gradients\n",
    "            \n",
    "        if bias is not None and context.needs_input_grad[4]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "        \n",
    "        if context.algorithm == 'YY':\n",
    "            return grad_input, None, grad_weight, grad_weight_feedback, grad_bias\n",
    "        else:\n",
    "            return grad_input, None, grad_weight, None, grad_bias\n",
    "\n",
    "class Linear2(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    a linear layer with asymmetric feedback and feedforward pathways\n",
    "    forward  : weight\n",
    "    backward : weight_feedback\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_features, output_features, bias, algorithm ):     # we ignore bias for now\n",
    "        \n",
    "        super(Linear2, self).__init__()\n",
    "        implemented_algorithms = ['BP', 'FA', 'YY']\n",
    "        assert algorithm in implemented_algorithms, 'feedback algorithm %s is not implemented'\n",
    "\n",
    "        \n",
    "        # self.input_features = input_features\n",
    "        # self.output_features = output_features\n",
    "        self.algorithm = algorithm\n",
    "        # weight and bias for forward pass\n",
    "        # weight has transposed form for efficiency (?) (transposed at forward pass)\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))\n",
    "        # as in torchvision/nn/modules/linear scaling was based on weight input (weight.size(1))\n",
    "        # since  weight_feedback is the transpose scaling should be like below\n",
    "#         self.scale_feedback = 1. / math.sqrt(self.weight.size(0))\n",
    "        if bias:  \n",
    "            self.bias = nn.Parameter(torch.Tensor(output_features))\n",
    "        else: \n",
    "            self.register_parameter('bias', None)\n",
    "        if self.algorithm == 'YY':\n",
    "            back_requires_grad = True\n",
    "        else:\n",
    "            back_requires_grad = False\n",
    "    \n",
    "        self.weight_feedback = nn.Parameter(torch.Tensor(output_features, input_features), \n",
    "                                            requires_grad=back_requires_grad)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        if self.algorithm == 'BP':\n",
    "            self.weight_feedback.data = copy.deepcopy(self.weight.detach())\n",
    "\n",
    "\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        init.kaiming_uniform_(self.weight_feedback, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input, input2):\n",
    "\n",
    "        # if self.algorithm == 'FA':\n",
    "\n",
    "        #     weight_feedback = self.weight_feedback\n",
    "        if self.algorithm == 'BP':\n",
    "\n",
    "            self.weight_feedback.data = copy.deepcopy(self.weight.detach())\n",
    "\n",
    "\n",
    "        return LinearFunction2.apply(input, input2, self.weight, self.weight_feedback, self.bias, self.algorithm)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
