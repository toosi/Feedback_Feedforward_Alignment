{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 16, 9, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 3, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import autograd\n",
    "import torch.nn as nn\n",
    "from modules import customized_modules_layerwise as customized_modules\n",
    "ConvTranspose2d = customized_modules.AsymmetricFeedbackConvTranspose2d\n",
    "\n",
    "# net = nn.ConvTranspose2d(64,16, 3, bias=None,)\n",
    "net = ConvTranspose2d(64,16, 3, bias=None, algorithm='IA')\n",
    "inputs = torch.randn(256, 64, 7, 7)\n",
    "targets = torch.randn(256, 16, 9, 9)\n",
    "outputs = net(inputs)\n",
    "print(outputs.shape)\n",
    "loss = nn.MSELoss()(outputs , targets)\n",
    "#loss.backward()\n",
    "\n",
    "autograd.grad(loss, net.weight)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "runname='Apr21-14-25_MNIST_9578f81f20_322'\n",
    "class Args:\n",
    "    config_file = '/home/tt2684/Research/Results/Symbio/Symbio/%s/configs.yml'%runname\n",
    "    method = 'SLVanilla'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'archd': 'AsymResLNet10B',\n",
      "    'arche': 'AsymResLNet10F',\n",
      "    'base_channels': 64,\n",
      "    'batch_size': 256,\n",
      "    'customdatasetdir_train': '/home/tt2684/Research/Data/Custom_datasets/MNIST/',\n",
      "    'databasedir': '/home/tt2684/Research/Results/database/Symbio/EAsymResLNet10FDAsymResLNet10B/MNIST/',\n",
      "    'dataset': 'MNIST',\n",
      "    'dist_backend': 'nccl',\n",
      "    'dist_url': 'tcp://224.66.41.62:23456',\n",
      "    'epochs': 150,\n",
      "    'evaluate': False,\n",
      "    'factord': 0.1,\n",
      "    'factore': 0.1,\n",
      "    'gamma': 0.001,\n",
      "    'gpu': None,\n",
      "    'hash': None,\n",
      "    'imagesetdir': '/home/tt2684/Research/Data/MNIST/',\n",
      "    'input_size': 32,\n",
      "    'loadinitialization': '',\n",
      "    'lossfuncB': 'MSELoss',\n",
      "    'lrB': 0.001,\n",
      "    'lrF': 0.001,\n",
      "    'momentum': 0.9,\n",
      "    'multiprocessing_distributed': False,\n",
      "    'n_classes': 10,\n",
      "    'note': '**modules_layerwise_denom_for_both=1**',\n",
      "    'offset': 10,\n",
      "    'optimizerB': 'RMSprop',\n",
      "    'optimizerF': 'RMSprop',\n",
      "    'path_prefix': '/home/tt2684/Research',\n",
      "    'path_save_model': '/home/tt2684/Research/Models/MNIST_trained/Symbio/EAsymResLNet10FDAsymResLNet10B/Apr21-14-25_MNIST_9578f81f20_322/',\n",
      "    'patienced': 40,\n",
      "    'patiencee': 50.0,\n",
      "    'pretrained': False,\n",
      "    'print_freq': 100,\n",
      "    'rank': -1,\n",
      "    'resultsdir': '/home/tt2684/Research/Results/Symbio/Symbio/Apr21-14-25_MNIST_9578f81f20_322/',\n",
      "    'resume': '',\n",
      "    'runname': 'Apr21-14-25_MNIST_9578f81f20_322',\n",
      "    'seed': None,\n",
      "    'start_epoch': 0,\n",
      "    'step': 100,\n",
      "    'tensorboarddir': '/home/tt2684/Research/Results/Tensorboard_runs/runs/Symbio/Apr21-14-25_MNIST_9578f81f20_322',\n",
      "    'time': 'Now',\n",
      "    'wdB': 1e-06,\n",
      "    'wdF': 1e-05,\n",
      "    'workers': 24,\n",
      "    'world_size': -1}\n",
      "SLVanilla\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import yaml \n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import scipy\n",
    "import h5py\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "matplotlib.use('agg')\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "from utils import state_dict_utils\n",
    "\n",
    "import pytorch_ssim\n",
    "\n",
    "\n",
    "# # toggle_state_dict = state_dict_utils.toggle_state_dict # for ResNetLraveled\n",
    "# toggle_state_dict = state_dict_utils.toggle_state_dict_resnets # for custom_resnets\n",
    "# toggle_state_dict_YYtoBP = state_dict_utils.toggle_state_dict_YYtoBP\n",
    "\n",
    "# # from models import custom_models_ResNetLraveled as custom_models\n",
    "# from models import custom_resnets as custom_models\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "import socket\n",
    "if socket.gethostname()[0:4] in  ['node','holm','wats']:\n",
    "    path_prefix = '/rigel/issa/users/Tahereh/Research'\n",
    "elif socket.gethostname() == 'SYNPAI':\n",
    "    path_prefix = '/hdd6gig/Documents/Research'\n",
    "elif socket.gethostname()[0:2] == 'ax':\n",
    "    path_prefix = '/home/tt2684/Research'\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='PyTorch Training')\n",
    "# parser.add_argument(\n",
    "#         '--config-file',\n",
    "#         dest='config_file',\n",
    "#         type=argparse.FileType(mode='r'))\n",
    "\n",
    "\n",
    "# parser.add_argument('--method', type=str, default='BP', metavar='M',\n",
    "#                     help='method:BP|SLVanilla|SLBP|FA|SLTemplateGenerator')\n",
    "\n",
    "\n",
    "args = Args()\n",
    "assert args.config_file, 'Please specify a config file path'\n",
    "if args.config_file:\n",
    "    with open(args.config_file, 'r') as stream:\n",
    "        data = yaml.safe_load(stream)        \n",
    "#     delattr(args, 'config_file')\n",
    "    arg_dict = args.__dict__\n",
    "    for key, value in data.items():\n",
    "        setattr(args, key, value)\n",
    "\n",
    "pp.pprint(arg_dict)\n",
    "print(args.method)\n",
    "with open(args.resultsdir+'args.yml', 'w') as outfile:\n",
    "    \n",
    "    yaml.dump(vars(args), outfile, default_flow_style=False)\n",
    "\n",
    "writer = SummaryWriter(log_dir=args.tensorboarddir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'AsymResLNet' in args.arche:\n",
    "    toggle_state_dict = state_dict_utils.toggle_state_dict_normalize\n",
    "    from models import custom_models_ResNetLraveled as custom_models\n",
    "\n",
    "elif 'asymresnet' in args.arche:\n",
    "    toggle_state_dict = state_dict_utils.toggle_state_dict_resnets\n",
    "    from models import custom_resnets as custom_models\n",
    "\n",
    "elif args.arche.startswith('resnet'):\n",
    "    from models import resnets as custom_models\n",
    "    #just for compatibality\n",
    "    toggle_state_dict = state_dict_utils.toggle_state_dict_resnets\n",
    "\n",
    "toggle_state_dict_YYtoBP = state_dict_utils.toggle_state_dict_YYtoBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST\n",
    "args.n_classes = 10\n",
    "if args.input_size is None:\n",
    "    input_size = 32\n",
    "else:\n",
    "    input_size = args.input_size\n",
    "image_channels = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_modelF = {'algorithm': args.method, 'base_channels':args.base_channels, 'image_channels':image_channels, 'n_classes':args.n_classes}\n",
    "args_modelB ={'algorithm': 'FA','base_channels':args.base_channels, 'image_channels':image_channels, 'n_classes':args.n_classes}\n",
    "\n",
    "modelF = getattr(custom_models, args.arche)(**args_modelF)\n",
    "modelB = getattr(custom_models, args.archd)(**args_modelB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AsymResLNet10B(\n",
       "  (upsample2): AsymmetricFeedbackConvTranspose2d(10, 128, kernel_size=(1, 1), stride=(2, 2), output_padding=(1, 1), bias=False)\n",
       "  (bn42): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (conv42): AsymmetricFeedbackConvTranspose2d(10, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (bn41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (conv41): AsymmetricFeedbackConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn32): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (conv32): AsymmetricFeedbackConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn31): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (conv31): AsymmetricFeedbackConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "  (bn23): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (upsample1): AsymmetricFeedbackConvTranspose2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (conv22): AsymmetricFeedbackConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn21): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (conv21): AsymmetricFeedbackConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (conv12): AsymmetricFeedbackConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (conv11): AsymmetricFeedbackConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (conv1): AsymmetricFeedbackConvTranspose2d(64, 1, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), output_padding=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(256, 1, 32, 32)\n",
    "targets = torch.randn(256, 1, 34, 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents, outputs = modelF(inputs)\n",
    "preconv1, recons = modelB(latents.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1, 34, 34])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 64, 16, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preconv1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()(recons, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RuntimeError: Given transposed=1, weight of size 2097152 1 16 16, expected input[1, 32768, 16, 16] to have 2097152 channels, but got 32768 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16384/256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 16, 9, 9])\n",
      "before ravel: torch.Size([256, 64, 7, 7]) torch.Size([256, 16, 9, 9]) 16384\n",
      "after ravel: torch.Size([1, 16384, 7, 7]) torch.Size([262144, 1, 9, 9]) 16384\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=16384, weight of size 16384 1 7 7, expected input[262144, 1, 9, 9] to have 16384 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ce35c0d34858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/conda-envs/pytorch_tensorflow_latest/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    147\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda-envs/pytorch_tensorflow_latest/lib/python3.7/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/issa/users/tt2684/Codes/Symbio/modules/customized_modules_layerwise.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(context, grad_output)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             grad_weight = F.conv2d(grad_output, inputs.permute(1,0,2,3), bias=None, stride=dilation, padding=padding,\n\u001b[0;32m--> 258\u001b[0;31m                                     dilation=stride, groups=in_channels * min_batch)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             grad_weight = grad_weight.contiguous().view(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=16384, weight of size 16384 1 7 7, expected input[262144, 1, 9, 9] to have 16384 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ravel: torch.Size([256, 3, 32, 32]) torch.Size([256, 16, 28, 28]) 768\n",
      "after ravel: torch.Size([1, 768, 32, 32]) torch.Size([12288, 1, 28, 28]) 768\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from modules import customized_modules_layerwise as customized_modules\n",
    "Conv2d = customized_modules.AsymmetricFeedbackConv2d\n",
    "net = Conv2d(3,16,5, bias=None, algorithm='FA')\n",
    "inputs = torch.randn(256, 3, 32, 32)\n",
    "targets = torch.randn(256, 16, 28, 28)\n",
    "outputs = net(inputs)\n",
    "loss = nn.MSELoss()(outputs , targets)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
