{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing te decoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import custom_models\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import scipy\n",
    "import h5py\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "if socket.gethostname()[0:4] in  ['node','holm','wats']:\n",
    "    path_prefix = '/rigel/issa/users/Tahereh/Research'\n",
    "elif socket.gethostname() == 'SYNPAI':\n",
    "    path_prefix = '/hdd6gig/Documents/Research'\n",
    "elif socket.gethostname()[0:2] == 'ax':\n",
    "    path_prefix = '/scratch/issa/users/tt2684/Research'\n",
    "    plt.switch_backend('agg')\n",
    "elif socket.gethostname() == 'turing':\n",
    "    path_prefix = '/home/tahereh/Documents/Research'\n",
    "\n",
    "\n",
    "imagesetdir = path_prefix+'/Data/'\n",
    "resultsdir = path_prefix+'/Results/Toy_models/'\n",
    "\n",
    "class Args:\n",
    "    dum=None\n",
    "\n",
    "args = Args()\n",
    "args.batch_size=256\n",
    "args.no_cuda = False\n",
    "args.seed = 0\n",
    "args.n_classes = 10\n",
    "args.dataset = 'MNIST'\n",
    "args.arche = 'FullyConnectedF'\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "use_cuda = True\n",
    "# data loader\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True, 'drop_last':True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(imagesetdir, train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.Resize(32),\n",
    "                        transforms.ToTensor(),\n",
    "                       \n",
    "                    ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(imagesetdir, train=False, transform=transforms.Compose([\n",
    "                        transforms.Resize(32),\n",
    "                        transforms.ToTensor(),\n",
    "                        \n",
    "                    ])),\n",
    "    batch_size=args.batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = custom_models.FullyConnectedF(hidden_layers=[256,256,10], nonlinearfunc='relu', input_length=32*32, algorithm='FA').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('/home/tahereh/Documents/Research/Results/Symbio/Symbio/Oct02-21-26_MNIST_e6507d0d57_RMSpropRMSpropMNISTFullyConnE150_805/checkpointe_BP.pth.tar')['state_dict']\n",
    "# state_dict = torch.load('/home/tahereh/Documents/Research/Results/Symbio/Symbio/Oct03-07-02_MNIST_30fb700eaa_None_519/checkpointe_BP.pth.tar')['state_dict']\n",
    "\n",
    "\n",
    "state_dict_model = {}\n",
    "for k in state_dict.keys():\n",
    "    state_dict_model.update({k.replace('module.',''):state_dict[k]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "odict_keys([&#39;module.fc_0.weight&#39;, &#39;module.fc_0.weight_feedback&#39;, &#39;module.fc_1.weight&#39;, &#39;module.fc_1.weight_feedback&#39;, &#39;module.fc_2.weight&#39;, &#39;module.fc_2.weight_feedback&#39;])"
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;All keys matched successfully&gt;"
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "model.load_state_dict(state_dict_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "orig 98.828125\ndecoder 98.828125\ndecoder 98.828125\ndecoder 98.828125\ndecoder 98.828125\ndecoder 99.21875\norig 97.65625\ndecoder 98.046875\ndecoder 97.65625\ndecoder 97.65625\ndecoder 97.65625\ndecoder 97.65625\norig 98.828125\ndecoder 98.4375\ndecoder 98.4375\ndecoder 98.4375\ndecoder 98.4375\ndecoder 98.4375\norig 98.4375\ndecoder 98.046875\ndecoder 98.046875\ndecoder 98.046875\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m&lt;ipython-input-105-4125c6ae8128&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mimagesD\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mimagesD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#images= images.expand(-1, 3, -1, -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 35\u001b[0;31m             \u001b[0mlatentsD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagesD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0moptimizerD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_tensorflow_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--&gt; 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/Symbio/models/custom_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonlinearfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m&#39;fc_%d&#39;\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 27\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m&#39;fc_%d&#39;\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;31m# just to satisfy the requirements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_tensorflow_latest/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--&gt; 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/Symbio/modules/BiHebb_modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--&gt; 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLinearFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_feedback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/Symbio/modules/BiHebb_modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(context, input, weight, weight_feedback, bias, algorithm)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_feedback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 66\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "# with torch.no_grad():\n",
    "for i, (images, target) in enumerate(test_loader):\n",
    "\n",
    "\n",
    "    images = images.cuda()\n",
    "    target = target.cuda()\n",
    "    \n",
    "    \n",
    "    images = images.expand(-1, 1, -1, -1) #images.expand(-1, 3, -1, -1)\n",
    "    \n",
    "    # ----- encoder ---------------------\n",
    "                \n",
    "    # compute output\n",
    "    latents, o = model(images)\n",
    "    acc1, acc5 = accuracy(o, target, topk=(1, 5))\n",
    "    print('orig',acc1[0].item())\n",
    "\n",
    "    n_latents = latents.view(latents.shape[0], -1).shape[-1]\n",
    "    decoder = nn.Linear(n_latents, args.n_classes).cuda()\n",
    "    optimizerD = torch.optim.SGD(decoder.parameters(), lr=0.1, weight_decay=1e-3)\n",
    "    criterionD = nn.CrossEntropyLoss()\n",
    "\n",
    "    for ep in range(5):\n",
    "        running_lossD = 0\n",
    "        for iD, (imagesD, targetD) in enumerate(train_loader):\n",
    "        \n",
    "            imagesD = imagesD.cuda()\n",
    "            targetD = targetD.cuda()   \n",
    "\n",
    "        \n",
    "            if ('MNIST' in args.dataset) and args.arche[0:2]!='FC':\n",
    "                imagesD= imagesD.expand(-1, 1, -1, -1) #images= images.expand(-1, 3, -1, -1) \n",
    "\n",
    "            latentsD, _ = model(imagesD)\n",
    "\n",
    "            optimizerD.zero_grad()\n",
    "            outputsD = decoder(latentsD.view(latentsD.shape[0], -1).detach())\n",
    "            lossD = criterionD(outputsD, targetD)\n",
    "            lossD.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "            running_lossD += lossD.item()\n",
    "\n",
    "        # print(running_lossD/(iD+1))\n",
    "\n",
    "        output = decoder(latents.view(latents.shape[0], -1).detach())\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        print('decoder', acc1[0].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;__main__.Args at 0x7fe46523f490&gt;"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Testing BP in modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autograd\n",
    "import torch.nn as nn\n",
    "from modules import BiHebb_modules as customized_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear = customized_modules.Linear\n",
    "net = Linear(64, 16, False, 'FA')\n",
    "inputs = torch.randn(256, 64)\n",
    "targets = torch.randn(256, 16)\n",
    "outputs = net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([[ 0.0685,  0.1118, -0.0390,  ..., -0.1008, -0.0246, -0.0348],\n        [ 0.0572, -0.0658,  0.0760,  ...,  0.1161,  0.0440, -0.0455],\n        [ 0.0815,  0.0741,  0.0345,  ...,  0.0596, -0.0076, -0.0036],\n        ...,\n        [ 0.0339, -0.0810, -0.0373,  ...,  0.1171,  0.0035,  0.0786],\n        [-0.0708, -0.0919, -0.0110,  ..., -0.0797, -0.1178, -0.0891],\n        [ 0.0156,  0.0254,  0.0426,  ...,  0.0513, -0.0196,  0.1123]],\n       requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "net.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([[ 0.0572, -0.0218,  0.0713,  ..., -0.0719, -0.0940, -0.1124],\n        [-0.1021,  0.0163, -0.0337,  ...,  0.0570,  0.0086,  0.0826],\n        [-0.0693, -0.0021,  0.0284,  ...,  0.0683,  0.0405,  0.1027],\n        ...,\n        [-0.0033, -0.0145, -0.0922,  ..., -0.0150, -0.0409, -0.0853],\n        [-0.0288,  0.0877,  0.1063,  ..., -0.0343, -0.0526, -0.0030],\n        [ 0.0782,  0.1013, -0.0678,  ...,  0.1000, -0.1146,  0.0892]])"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "net.weight_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.0041, -0.0059, -0.0162],\n        [-0.0341,  0.0090,  0.0318],\n        [ 0.0396, -0.0045, -0.0339]], grad_fn=&lt;SelectBackward&gt;)"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "Conv2d = customized_modules.Conv2d\n",
    "net = Conv2d(64, 16, 3, algorithm='FA')\n",
    "inputs = torch.randn(256, 64, 32, 32)\n",
    "outputs = net(inputs)\n",
    "net.weight[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.0350,  0.0119, -0.0236],\n        [ 0.0006, -0.0341, -0.0392],\n        [-0.0068,  0.0342,  0.0386]])"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "net.weight_feedback[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Target Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autograd\n",
    "import torch.nn as nn\n",
    "from modules import TP_modules as customized_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([256, 16])\n"
    }
   ],
   "source": [
    "Linear = customized_modules.Linear\n",
    "net = Linear(64, 16, False, 'TP')\n",
    "inputs = torch.randn(256, 64)\n",
    "targets = torch.randn(256, 16)\n",
    "outputs = net(inputs)\n",
    "print(outputs.shape)\n",
    "loss = nn.MSELoss()(outputs , targets)\n",
    "loss.backward()\n",
    "\n",
    "# autograd.grad(loss, net.weight)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import scipy\n",
    "import h5py\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "if socket.gethostname()[0:4] in  ['node','holm','wats']:\n",
    "    path_prefix = '/rigel/issa/users/Tahereh/Research'\n",
    "elif socket.gethostname() == 'SYNPAI':\n",
    "    path_prefix = '/hdd6gig/Documents/Research'\n",
    "elif socket.gethostname()[0:2] == 'ax':\n",
    "    path_prefix = '/scratch/issa/users/tt2684/Research'\n",
    "    plt.switch_backend('agg')\n",
    "elif socket.gethostname() == 'turing':\n",
    "    path_prefix = '/home/tahereh/Documents/Research'\n",
    "\n",
    "\n",
    "imagesetdir = path_prefix+'/Data/'\n",
    "resultsdir = path_prefix+'/Results/Toy_models/'\n",
    "\n",
    "class Args:\n",
    "    dum=None\n",
    "\n",
    "args = Args()\n",
    "args.batch_size=256\n",
    "args.algorithm = 'TP' #'TP' #'BP'\n",
    "args.no_cuda = False\n",
    "args.seed = 0\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "use_cuda = True\n",
    "# data loader\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True, 'drop_last':True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(imagesetdir, train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.Resize(32),\n",
    "                        transforms.ToTensor(),\n",
    "                       \n",
    "                    ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(imagesetdir, train=False, transform=transforms.Compose([\n",
    "\n",
    "                        transforms.Resize(32),\n",
    "                        transforms.ToTensor(),\n",
    "                        \n",
    "                    ])),\n",
    "    batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "# transforms.Normalize((0.1307,), (0.3081,))\n",
    "n_layers = 3\n",
    "algorithm = args.algorithm #'inverse'#'forward'#'inverse'#'BP'\n",
    "n_dataloader = 100000\n",
    "batch_size = args.batch_size\n",
    "\n",
    "\n",
    "class ReLUGrad(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLUGrad, self).__init__()\n",
    "    def forward(self, grad_output, input):\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "class Forward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Forward, self).__init__()\n",
    "        self.fc_0 = Linear(1024, 256, bias=False, algorithm=algorithm)\n",
    "        self.fc_1 = Linear(256, 10, bias=False, algorithm=algorithm)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        xrelu = self.fc_0(x)\n",
    "        x0 = self.relu(xrelu)\n",
    "        x1 = self.fc_1(x0)\n",
    "\n",
    "        return x1 #, [x, x0, x1], xrelu\n",
    "\n",
    "class Backward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Backward, self).__init__()\n",
    "        self.fc_1 = Linear(10, 256, bias=False, algorithm=algorithm)\n",
    "        self.fc_0 = Linear(256, 1024, bias=False, algorithm=algorithm)\n",
    "        self.grelu = ReLUGrad()\n",
    "\n",
    "    def forward(self, x, x0):\n",
    "        x1 = self.fc_1(x)\n",
    "        x1 = self.grelu(x1, x0)\n",
    "        x0 = self.fc_0(x1)\n",
    "\n",
    "        return x0#, [x1,  x]\n",
    "\n",
    "def transpose_weights(state_dict):\n",
    "\n",
    "    state_dict_new = {}\n",
    "    for k, item in state_dict.items():\n",
    "        state_dict_new.update({k: item.t()})\n",
    "    return state_dict_new\n",
    "\n",
    "\n",
    "\n",
    "modelF = Forward().cuda() # main model\n",
    "modelB = Backward().cuda() # backward network to compute gradients for modelF\n",
    "\n",
    "modelC = Forward().cuda() # Forward Control model to compare to BP\n",
    "modelC.load_state_dict(modelF.state_dict())\n",
    "# start symmetric\n",
    "# modelB.load_state_dict(transpose_weights(modelF.state_dict()) )\n",
    "modelE = Forward().cuda()\n",
    "\n",
    "optimizerC = optim.RMSprop(modelC.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "optimizerF = optim.RMSprop([p for n,p in modelF.named_parameters() if 'feedback' not in n],  lr=1e-3, weight_decay=1e-6)\n",
    "optimizerB = optim.RMSprop([p for n,p in modelB.named_parameters() if 'feedback' not in n],  lr=1e-3, weight_decay=1e-6)\n",
    "optimizerB_TP = optim.RMSprop([p for n,p in modelF.named_parameters() if 'feedback' in n],  lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "\n",
    "criterionF = nn.CrossEntropyLoss() #\n",
    "# criterionB = nn.MSELoss() #\n",
    "\n",
    "n_classes = 10\n",
    "onehot = torch.zeros(train_loader.batch_size, n_classes).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 0.5676737737197143\n1 0.567823022349268\n2 0.5681442454075202\n3 0.5692038304275937\n4 0.5727376921309365\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m&lt;ipython-input-70-348f7d3885a1&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----&gt; 5\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0monehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for images, targets in train_loader:\n",
    "        images = images.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        onehot = torch.zeros(train_loader.batch_size, n_classes).cuda()\n",
    "        onehot.zero_()\n",
    "        onehot.scatter_(1, targets.view(train_loader.batch_size,-1), 1)\n",
    "        onehot.requires_grad = True\n",
    "\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        out = modelF(images)\n",
    "        loss = criterionF(out, targets)\n",
    "        optimizerB_TP.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizerB_TP.step()\n",
    "\n",
    "        \n",
    "        optimizerF.zero_grad()\n",
    "        onehot.backward(torch.ones_like(onehot))\n",
    "        optimizerF.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(epoch, running_loss/len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for images, targets in train_loader:\n",
    "        images = images.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "    \n",
    "\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        out = modelF(images)\n",
    "        loss = criterionF(out, targets)\n",
    "\n",
    "        \n",
    "        optimizerF.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizerF.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(epoch, running_loss/len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}