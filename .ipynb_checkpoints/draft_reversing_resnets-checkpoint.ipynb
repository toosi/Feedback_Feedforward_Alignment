{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from utils import cifar10p1_OOD\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tt2684/Research/Data/CIFAR10_1/cifar10.1-labels.npy\n",
      "Files already downloaded and verified\n",
      "/home/tt2684/Research/Data/CIFAR10_1/cifar10.1-labels.npy\n"
     ]
    }
   ],
   "source": [
    "input_size = 32\n",
    "train_mean = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "train_std = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "\n",
    "class Args():\n",
    "    pass \n",
    "\n",
    "\n",
    "args = Args()\n",
    "args.dataset = 'CIFAR10_1'\n",
    "args.imagesetdir = '/home/tt2684/Research/Data/CIFAR10_1/'\n",
    "args.batch_size=256\n",
    "args.workers = 4\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "transforms.Resize(input_size),\n",
    "transforms.RandomCrop(input_size, padding=4),\n",
    "# transforms.RandomAffine(degrees=30),\n",
    "transforms.RandomHorizontalFlip(),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize(train_mean, train_std),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std),\n",
    "])\n",
    "\n",
    "\n",
    "test_dataset = getattr(cifar10p1_OOD, args.dataset)(root=args.imagesetdir, download=True, transform=transform_test)\n",
    "val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(val_loader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b3a30027f28>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[3].permute([1,2,0]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import custom_resnets as custom_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3320,  0.0632, -0.0421], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Linear(4,3, bias=False)\n",
    "m(torch.rand(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReLUGrad(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLUGrad, self).__init__()\n",
    "    def forward(self, grad_output, input):\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.ReLU()\n",
    "inputs = torch.rand(2,3)\n",
    "\n",
    "b = ReLUGrad()\n",
    "error = nn.Parameter(torch.tensor([1.1,2.1]), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1397, 0.7951, 0.6761],\n",
       "        [0.0495, 0.2153, 0.5409]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fcdfa0bedd7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/conda-envs/pytorch_tensorflow_latest/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda-envs/pytorch_tensorflow_latest/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda-envs/pytorch_tensorflow_latest/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "error.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.BatchNorm2d(64)\n",
    "inputs = torch.rand(1,64,32, 32)\n",
    "outputs = m(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(m.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-2b161b87d57e>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-2b161b87d57e>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class GBatchNorm2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GBatchNorm2d, self).__init__()\n",
    "    def forward(self, grad_output, cache):\n",
    "        #unfold the variables stored in cache\n",
    "        xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\n",
    "\n",
    "        #get the dimensions of the input/output\n",
    "        N,D = dout.shape\n",
    "\n",
    "        #step9\n",
    "        dbeta = np.sum(dout, axis=0)\n",
    "        dgammax = dout #not necessary, but more understandable\n",
    "\n",
    "        #step8\n",
    "        dgamma = np.sum(dgammax*xhat, axis=0)\n",
    "        dxhat = dgammax * gamma\n",
    "\n",
    "        #step7\n",
    "        divar = np.sum(dxhat*xmu, axis=0)\n",
    "        dxmu1 = dxhat * ivar\n",
    "\n",
    "        #step6\n",
    "        dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "\n",
    "        #step5\n",
    "        dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
    "\n",
    "        #step4\n",
    "        dsq = 1. /N * np.ones((N,D)) * dvar\n",
    "\n",
    "        #step3\n",
    "        dxmu2 = 2 * xmu * dsq\n",
    "\n",
    "        #step2\n",
    "        dx1 = (dxmu1 + dxmu2)\n",
    "        dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n",
    "\n",
    "        #step1\n",
    "        dx2 = 1. /N * np.ones((N,D)) * dmu\n",
    "\n",
    "        #step0\n",
    "        dx = dx1 + dx2\n",
    "\n",
    "        return dx, dgamma, dbeta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBN1(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1,\n",
    "                 affine=True, track_running_stats=True):\n",
    "        super(MyBN1, self).__init__(\n",
    "            num_features, eps, momentum, affine, track_running_stats)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self._check_input_dim(input)\n",
    "\n",
    "        exponential_average_factor = 0.0\n",
    "\n",
    "        if self.training and self.track_running_stats:\n",
    "            if self.num_batches_tracked is not None:\n",
    "                self.num_batches_tracked += 1\n",
    "                if self.momentum is None:  # use cumulative moving average\n",
    "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
    "                else:  # use exponential moving average\n",
    "                    exponential_average_factor = self.momentum\n",
    "\n",
    "        # calculate running estimates\n",
    "        if self.training:\n",
    "            mean = input.mean([0, 2, 3])\n",
    "            var = input.var([0, 2, 3], unbiased=False)\n",
    "            n = input.numel() / input.size(1)\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = exponential_average_factor * mean\\\n",
    "                    + (1 - exponential_average_factor) * self.running_mean\n",
    "                # update running_var with unbiased var\n",
    "                self.running_var = exponential_average_factor * var * n / (n - 1)\\\n",
    "                    + (1 - exponential_average_factor) * self.running_var\n",
    "            mean = mean.view(1, -1, 1, 1)\n",
    "            var = var.view(1, -1, 1, 1)\n",
    "                \n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        weight = self.weight.view(1, -1, 1, 1)\n",
    "        bias = self.bias.view(1, -1, 1, 1)\n",
    "\n",
    "        output = MyBNFunc.apply(input, mean, var, weight, bias, self.eps)\n",
    "        \n",
    "        return output\n",
    "\n",
    "def mysum(tensor):\n",
    "    return tensor.sum((0,2,3),keepdim=True)\n",
    "class MyBNFunc(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, avg, var, gamma, beta, eps):\n",
    "        ctx.avg = avg\n",
    "        ctx.var = var\n",
    "        ctx.eps = eps\n",
    "        ctx.shape = input.shape\n",
    "        output = input - avg\n",
    "        scale = 1 / torch.sqrt(var + eps)\n",
    "        output = output * scale\n",
    "        ctx.save_for_backward(input, gamma, beta, output, scale)\n",
    "        output = output * gamma + beta\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, gamma, beta, output, scale = ctx.saved_tensors\n",
    "        avg = ctx.avg\n",
    "        var = ctx.var\n",
    "        eps = ctx.eps\n",
    "        B, C, H, W = ctx.shape\n",
    "\n",
    "        dL_dxi_hat = grad_output * gamma\n",
    "        dl_dvar = mysum(dL_dxi_hat) * (input - avg) * -0.5 * scale * scale * scale\n",
    "        dl_davg = mysum(dL_dxi_hat * -1.0 * scale) + dl_dvar * mysum(-2.0 * (input - avg)) / (B * H * W)\n",
    "        dL_dxi = dL_dxi_hat * scale + dl_dvar * 2.0 * (input - avg) / (B*H*W)  + dl_davg / (B*H*W)\n",
    "        dL_dgamma = (grad_output * output).sum((0, 2, 3), keepdim=True) \n",
    "        dL_dbeta = (grad_output).sum((0, 2, 3), keepdim=True)\n",
    "        return dL_dxi, dl_davg, dl_dvar, dL_dgamma, dL_dbeta, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AsymResNet(\n",
       "  (conv1): AsymmetricFeedbackConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AsymmetricFeedbackConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AsymmetricFeedbackConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AsymmetricFeedbackConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (downsample): AsymmetricFeedbackConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AsymmetricFeedbackConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AsymmetricFeedbackConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (downsample): AsymmetricFeedbackConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AsymmetricFeedbackConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AsymmetricFeedbackConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (downsample): AsymmetricFeedbackConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AsymmetricFeedbackConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (conv2): AsymmetricFeedbackConv2d(512, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_models.asymresnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AsymResNetT(\n",
       "  (conv2): AsymmetricFeedbackConvTranspose2d(10, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlockT(\n",
       "      (conv1): AsymmetricFeedbackConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    )\n",
       "    (1): BasicBlockT(\n",
       "      (conv1): AsymmetricFeedbackConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (upsample): AsymmetricFeedbackConvTranspose2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlockT(\n",
       "      (conv1): AsymmetricFeedbackConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    )\n",
       "    (1): BasicBlockT(\n",
       "      (conv1): AsymmetricFeedbackConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (upsample): AsymmetricFeedbackConvTranspose2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlockT(\n",
       "      (conv1): AsymmetricFeedbackConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    )\n",
       "    (1): BasicBlockT(\n",
       "      (conv1): AsymmetricFeedbackConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (upsample): AsymmetricFeedbackConvTranspose2d(128, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlockT(\n",
       "      (conv1): AsymmetricFeedbackConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    )\n",
       "    (1): BasicBlockT(\n",
       "      (conv1): AsymmetricFeedbackConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AsymmetricFeedbackConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (conv1): AsymmetricFeedbackConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_models.asymresnetT18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
